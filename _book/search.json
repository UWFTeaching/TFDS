[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tools for Data Science",
    "section": "",
    "text": "Welcome to TS4DS\nThis eBook is used for Tools and Statistics for Data Science (TS4DS) courses offered at [the University of West Florida, Mathematics and Statistics Department](https://uwf.edu/hmcse/departments/mathematics-and-statistics/){target=“_blank”}\nAn 8-Week course on Statistics for data science 1 using R or SAS.\nAn 8-Week course on Tools for data science using R, Python, and SQL. Throughout the course, there will be hands-on exercises with computing resources. The course will include introductions to several packages in R, particularly Tidyverse, libraries in Python such as Pandas/NumPy/Statsmodels, SQL clauses and summary statistics, and Spark framework for distributed computing.\n\nTopics\n\nIntroduction to R/RStudio\nProgramming using R\nIntroduction to Python\nProgramming using Python\nIntroduction to SQL\nData input and output\nData manipulation\nSummary statistics\nStatistical and mathematical functions\nGraphics and data visualization\nHypothesis testing\nRegression\nt-tests, ANOVA\nNonparametric tests\n\n\n\nReadings\nIn addition to material provided in this course, I highly encourage reading and reviewing some of the material, I will be pointing out throughout the course, including:\n\nR for Data Science (Wickham and Grolemund 2016). It is available free online.\nHands-On Programming with R (Grolemund 2014). It is available free online.\nExploring Enterprise Databases with R: A Tidyverse Approach (John David et al. 2020)\nMastering Spark with R (Luraschi, Kuo, and Ruiz 2019). It is available free online.\nPractical Guide for Oracle SQL, T-SQL and MySQL (Zhang 2017)\nThink Python (Downey 2015)\nData Science and Analytics with Python (Rogel-Salazar 2018)\n\n\n\n\n\nDowney, Allen. 2015. Think Python How to Think Like a Computer Scientist. Green Tea Press Needham, Massachusetts.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r: Write Your Own Functions and Simulations. \" O’Reilly Media, Inc.\".\n\n\nJohn David, Smith, Yang Sophie, Borasky M. Edward (Ed), Tyhurst Jim, Came Scott, Thygesen Mary Anne, and Frantz Ian. 2020. Exploring Enterprise Databases with r: A Tidyverse Approach. https://smithjd.github.io/sql-pet/.\n\n\nLuraschi, Javier, Kevin Kuo, and Edgar Ruiz. 2019. Mastering Spark with r: The Complete Guide to Large-Scale Analysis and Modeling. O’Reilly Media.\n\n\nRogel-Salazar, Jesus. 2018. Data Science and Analytics with Python. CRC Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\".\n\n\nZhang, Preston. 2017. Practical Guide for Oracle SQL, t-SQL and MySQL. CRC Press."
  },
  {
    "objectID": "t4ds.html",
    "href": "t4ds.html",
    "title": "Tools for Data Science",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee (knuth84?) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "t4ds/index.html",
    "href": "t4ds/index.html",
    "title": "Introduction",
    "section": "",
    "text": "An 8-Week course on tools for data science using R, Python, SQL, and Spark. Throughout the course, there will be hands-on exercises with computing resources. The course will include introductions to several packages in R, particularly Tidyverse, libraries in Python such as Pandas/NumPy/Statsmodels, SQL clauses and summary statistics, and Spark framework for distributed computing.\n\nLearning outcomes\n\nDevelop programming skills\nWrite codes using R, Python, and SQL\nShow the ability to manage, wrangle, and visualize data\nPractice with Spark for distributed processing\nCompare statistical summaries using SQL, R and Python\nCreate and evaluate scripts to answer data driven problems\n\n\n\nTopics\n\nIntroduction to R/RStudio\nProgramming using R\nIntroduction to Python\nProgramming using Python\nIntroduction to SQL\nIntroduction to Spark\nData input and output\nData manipulation\nSummary statistics\nStatistical and mathematical functions\nGraphics and data visualization\n\nNote that the focus of the course will be learning programming using R and Python from the very basics. We will explore the usage of SQL and Spark for querying and processing data. We will learn and practice coding concepts and programming practices that are universal but relevant to data science projects and workflow.\n\n\nOrganization and Evaluation\nEach week, we will have 2 tasks to complete as follows:\n\n\n\n\n\n\n\n\nDays\nTo do\nDue day\n\n\n\n\nMondays to Wednesdays\nReading/Watch recordings\nNA\n\n\nWednesdays to Mondays\nPractice and Discuss (credit 5%)\nMonday@11:59 pm CT\n\n\n\n\n\n\n\n\nEvery two weeks we will have an Individual project (credit 10%) to complete.\n\n\nReadings\nIn addition to material provided in this course, I highly encourage reading and reviewing some of the material, I will be pointing out throughout the course, including:\n\nR for Data Science (Wickham and Grolemund 2016). It is available free online.\nHands-On Programming with R (Grolemund 2014). It is available free online.\nExploring Enterprise Databases with R: A Tidyverse Approach (John David et al. 2020)\nMastering Spark with R (Luraschi, Kuo, and Ruiz 2019). It is available free online.\nPractical Guide for Oracle SQL, T-SQL and MySQL (Zhang 2017)\nThink Python (Downey 2015)\nData Science and Analytics with Python (Rogel-Salazar 2018)\n\n\n\n\n\nDowney, Allen. 2015. Think Python How to Think Like a Computer Scientist. Green Tea Press Needham, Massachusetts.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r: Write Your Own Functions and Simulations. \" O’Reilly Media, Inc.\".\n\n\nJohn David, Smith, Yang Sophie, Borasky M. Edward (Ed), Tyhurst Jim, Came Scott, Thygesen Mary Anne, and Frantz Ian. 2020. Exploring Enterprise Databases with r: A Tidyverse Approach. https://smithjd.github.io/sql-pet/.\n\n\nLuraschi, Javier, Kevin Kuo, and Edgar Ruiz. 2019. Mastering Spark with r: The Complete Guide to Large-Scale Analysis and Modeling. O’Reilly Media.\n\n\nRogel-Salazar, Jesus. 2018. Data Science and Analytics with Python. CRC Press.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\".\n\n\nZhang, Preston. 2017. Practical Guide for Oracle SQL, t-SQL and MySQL. CRC Press."
  },
  {
    "objectID": "t4ds/week1.html#data-science",
    "href": "t4ds/week1.html#data-science",
    "title": "R Basics",
    "section": "Data Science?",
    "text": "Data Science?\n\nThe use of data as evidence is crucial but, it is not something novel. If we examine a definition of the field of statistics, we can observe that is given as four subtopics:\n\nData Collection\nData Analysis\nResults Interpretation\nData Visualization\n\nOriginally, statistics was viewed as the analysis and interpretation of information about states. And science is understood as organized knowledge in the form of testable explanations and predictions about the universe.\nSo, what is data science? Data science is more than just using statistics and data to answer scientific questions.\n\nNowadays, data science is viewed as the use of various sources of data to extract knowledge and provide insights using multiple skills including programming, math and statistics, and communication.\n\nVenn diagram by Drew Conway provides a visualization on data science.\n\n\n\n\nData Science Venn diagram by Drew Conway\n\n\n\nTypical examples of data science projects:\n\nMarket analysis What product will sell better in conjunction with another popular product\nMarket segmentation Are there distinguishable features that characterize different groups of sales agents, customers or businesses?\nAdvertising and marketing What advertisement should be placed on what site?\nFraud How to detect if a retail/finance transaction is valid or not?\nDemand forecasting What is the demand for a particle service at a specific time/place?\nClassification Emails classification (spam vs. valid email)\n\n\nTools for Data Science\nData science helps managers, engineers, policymakers, and researchers - almost everybody - to make informed decisions based on evidence from data. Computers and technologies have empowered how much data we can store, manipulate, and analyze. To enable these functions, technologies and tools are developed to help us to be more productive and efficient when conducting data science projects.\n\n\n\nData Science Workflow\n\n\nThe technologies deployed in the analytics and data science have advanced very fast and multiple open source projects exist, for example:\n\nData framework: Hadoop, Spark,…\nQuery Languages: SQL, SQL-like,…\nData manipulation, modeling, and graphing: R, Python,…\nSoftware management: Git, GitHub,…\n\n\n\nData Science Workflow\nOften, the data science process is iterative. Some steps in the data science workflow include:\n\nSpecify the question of interest (business understanding, scientific goal, predict or estimate,…)\nCollect data (internal, external, sampled, relevant, ethics,…)\nManipulate data (explore, transform, merge, filter,…)\nModel data (machine learning, statistics, probability, fit, validate,…)\nCommunicate and interpret the results (storytelling, visualization, dashboard, reports,…)\nDeploy and monitor models"
  },
  {
    "objectID": "t4ds/week1.html#introduction-to-r-rstudio",
    "href": "t4ds/week1.html#introduction-to-r-rstudio",
    "title": "R Basics",
    "section": "Introduction to R / RStudio",
    "text": "Introduction to R / RStudio\nThe two programming languages we cover in this course are R and Python. These are both open source programming languages. Let’s start off with R.\nA few features of R are:\n\n\nR is a free software environment for statistical computing and graphics.\nIt compiles and runs on a wide variety of UNIX platforms, Windows and MacOS.\nThe R project website contains a lot of useful information about: download R, documentation and manuals, The R journal, books related to R, and R packages by topics\nThere is a large active community of R users.\n\n\n\nRStudio is an integrated development environment (IDE) for R and Python, with a console, syntax-highlighting editor that supports direct code execution, and tools for plotting, history, debugging and workspace management. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS. The RStudio website. There is an open source license for both the desktop and server versions that you install for free from here: download RStudio\n\n\nR Markdown provides an authoring framework for data science. You can use a single R Markdown file to both save and execute code generate high quality reports that can be shared with an audience. R Markdown documents are fully reproducible and support dozens of static and dynamic output formats. The R Markdown 1-minute video provides an overview of what R Markdown can do!\n\n\nInstall your R/RStudio\nFor TFDS, we will be using RStudio Server hosted at UWF. This is the link https://rstudio.hmcse.uwf.edu/. Login using your UWF account.\nYou don’t need to install R and RStudio on your computer. But, you are welcome to do so if you wish so."
  },
  {
    "objectID": "t4ds/week1.html#getting-started-with-r",
    "href": "t4ds/week1.html#getting-started-with-r",
    "title": "R Basics",
    "section": "Getting started with R",
    "text": "Getting started with R\n🛎️ Recordings of this week provide lessons about R, RStudio, and GitHub. The following will be covered:\n\nRStudio (editor, console, global Env., and etc.)\nR (scripts, packages, help)\nGitHub and connection to RStudio\nR Markdown - Cheet Sheet\nMy first R script - the basics\n\nValues, vectors, matrices, factors, data.frames, lists. Here is an example of code:\n\n\n\n# assign a value to object named \"x\"\nx = 1\n# or\nx &lt;- 1\n1 -&gt; x  \n# Calculator \nx=10^2\ny=2*x\n# vectors\nc(1,21,50,80,45,0)\n\n[1]  1 21 50 80 45  0\n\nc(\"d\",\"4\",\"r\")\n\n[1] \"d\" \"4\" \"r\"\n\n# characters\n\"R is useful and cool\"\n\n[1] \"R is useful and cool\"\n\n# boolean - TRUE or FALSE\n45&gt;96\n\n[1] FALSE\n\n# built-in functions\nsum(1,3,5)\n\n[1] 9\n\n\n\nStatistical and mathematical functions: An example of code:\n\n\n# a vector / array\nvec1= c(1,21,50,80,45,0)\n# minimun\nmin(vec1)\n\n[1] 0\n\n# maximum\nmax(vec1)\n\n[1] 80\n\n# exponential function\nexp(vec1)\n\n[1] 2.718282e+00 1.318816e+09 5.184706e+21 5.540622e+34 3.493427e+19\n[6] 1.000000e+00\n\n# cosine function\ncos(vec1)\n\n[1]  0.5403023 -0.5477293  0.9649660 -0.1103872  0.5253220  1.0000000\n\n# sine function\nsin(vec1)\n\n[1]  0.8414710  0.8366556 -0.2623749 -0.9938887  0.8509035  0.0000000\n\n# logarithm function of base e\nlog(vec1,0.5)\n\n[1]  0.000000 -4.392317 -5.643856 -6.321928 -5.491853       Inf\n\n# square root\nsqrt(vec1)\n\n[1] 1.000000 4.582576 7.071068 8.944272 6.708204 0.000000\n\n# logarithm function of base 10\nlog10(10)\n\n[1] 1\n\n# logarithm function of base 2\nlog2(2)\n\n[1] 1\n\n# logarithm function of base 45\nlog(45,base = 45)\n\n[1] 1\n\n# factorial\nfactorial(3)\n\n[1] 6\n\n# binomial coefficient / combination\nchoose(10,5)\n\n[1] 252\n\n\n\nSummary statistics, random number generation. An example:\n\n\n# a set of values\nvec1= c(1,21,50,80,45,0)\n# summation\nsum(vec1)\n\n[1] 197\n\n# arithmetic mean\nmean(vec1)\n\n[1] 32.83333\n\n# standard deviation\nsd(vec1)\n\n[1] 31.30122\n\n# summary statistics\nsummary(vec1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    6.00   33.00   32.83   48.75   80.00 \n\n# variance\nvar(vec1)\n\n[1] 979.7667\n\n# quantile\nquantile(vec1,0.5)\n\n50% \n 33 \n\n# 100 Standard normal random numbers\nx=rnorm(100,mean=0,sd=1)\n# histogram\nhist(x)\n\n\n\n\n\n\n\n\n\nFunctions, conditional statements: if, for and while. A code example:\n\n\n# create your own function\n  myfunction=function(){\n    return(print(\"Hello there!\"))\n  }\n# if statement\nlucky.number=100\nif(lucky.number&lt;=54){\nprint(\"You win!\")\n  }else{\n  print(\"You lost!\")\n}\n\n[1] \"You lost!\"\n\n\n🛎 🎙️ Recordings on Canvas will cover more details and examples! Have fun learning and coding 😃! Let me know how I can help!"
  },
  {
    "objectID": "t4ds/week1.html#assignment---r-basics",
    "href": "t4ds/week1.html#assignment---r-basics",
    "title": "R Basics",
    "section": "📚 👈 Assignment - R basics",
    "text": "📚 👈 Assignment - R basics\nInstructions are posted on Canvas."
  },
  {
    "objectID": "t4ds/week2.html#tidy-data",
    "href": "t4ds/week2.html#tidy-data",
    "title": "R Tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\nA data is said to be tidy (Wickham 2014) format if each column represents a variable and each row represents an observation. Example of data that is NOT tidy is the relig_income data set in tidyr package:\n# load a libraries\nlibrary(knitr) # fancy table\nlibrary(tidyverse) # load library tidyverse\n# To display fancy tables\nkable(head(relig_income,10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreligion\n&lt;$10k\n$10-20k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n$75-100k\n$100-150k\n&gt;150k\nDon’t know/refused\n\n\n\n\nAgnostic\n27\n34\n60\n81\n76\n137\n122\n109\n84\n96\n\n\nAtheist\n12\n27\n37\n52\n35\n70\n73\n59\n74\n76\n\n\nBuddhist\n27\n21\n30\n34\n33\n58\n62\n39\n53\n54\n\n\nCatholic\n418\n617\n732\n670\n638\n1116\n949\n792\n633\n1489\n\n\nDon’t know/refused\n15\n14\n15\n11\n10\n35\n21\n17\n18\n116\n\n\nEvangelical Prot\n575\n869\n1064\n982\n881\n1486\n949\n723\n414\n1529\n\n\nHindu\n1\n9\n7\n9\n11\n34\n47\n48\n54\n37\n\n\nHistorically Black Prot\n228\n244\n236\n238\n197\n223\n131\n81\n78\n339\n\n\nJehovah’s Witness\n20\n27\n24\n24\n21\n30\n15\n11\n6\n37\n\n\nJewish\n19\n19\n25\n25\n30\n95\n69\n87\n151\n162\n\n\n\n\nIt is obvious that each column does not represent a variable. Variable salary could be a better fit to the values we have in the columns headings (&lt;$10k, etc.). Another variable can be created to store values in the entry table (27, 34,…). These are the number of time we have a response - counts -. To make it tidy we need then to pivot the values columns into a two-column key-value pair. Let’s name the values in the header income and values in the table counts. To do that we can run the following code:\n# pivot a table/data frame\npivot_longer(relig_income,-religion,names_to='income',values_to = \"count\") -&gt; tidydata\n# To display fancy tables\nkable(head(tidydata,n = 12))\n\n\n\n\nreligion\nincome\ncount\n\n\n\n\nAgnostic\n&lt;$10k\n27\n\n\nAgnostic\n$10-20k\n34\n\n\nAgnostic\n$20-30k\n60\n\n\nAgnostic\n$30-40k\n81\n\n\nAgnostic\n$40-50k\n76\n\n\nAgnostic\n$50-75k\n137\n\n\nAgnostic\n$75-100k\n122\n\n\nAgnostic\n$100-150k\n109\n\n\nAgnostic\n&gt;150k\n84\n\n\nAgnostic\nDon’t know/refused\n96\n\n\nAtheist\n&lt;$10k\n12\n\n\nAtheist\n$10-20k\n27"
  },
  {
    "objectID": "t4ds/week2.html#manipulating-data",
    "href": "t4ds/week2.html#manipulating-data",
    "title": "R Tidyverse",
    "section": "Manipulating data",
    "text": "Manipulating data\ndplyr package is designed to perform some of the widely used operations when working with data.frame or tibble. - The dplyr Cheet Sheet. When manipulating data, you may want to:\n\nSubset the data to contain only row (observations) you are interested in\nSubset the data to contain only columns (variables) you are interested in\nCreate new variables and add them to the data\naggregate the data\n\nTo achieve these operations and more, the package dplyroffers the following functions:\n\n\n\nFunction\nAction\n\n\n\n\nfilter()\nsubset rows\n\n\nselect()\nsubset variables\n\n\nmutate()\ncreate a new variable\n\n\narrange()\nsort\n\n\nsummarize()\naggregate the data\n\n\n———-\n———\n\n\n\nHere is an example:\n# pivot a table/data frame\npivot_longer(relig_income,-religion,names_to='income',values_to = \"count\") -&gt; tidydata\n# Select data where income is &lt; $10k\nkable(head(filter(tidydata,income==\"&lt;$10k\")))\n\n\n\n\nreligion\nincome\ncount\n\n\n\n\nAgnostic\n&lt;$10k\n27\n\n\nAtheist\n&lt;$10k\n12\n\n\nBuddhist\n&lt;$10k\n27\n\n\nCatholic\n&lt;$10k\n418\n\n\nDon’t know/refused\n&lt;$10k\n15\n\n\nEvangelical Prot\n&lt;$10k\n575\n\n\n\n\n# Select data where income is &lt; $10k\nkable(head(arrange(tidydata,desc(count))))\n\n\n\n\nreligion\nincome\ncount\n\n\n\n\nEvangelical Prot\nDon’t know/refused\n1529\n\n\nCatholic\nDon’t know/refused\n1489\n\n\nEvangelical Prot\n$50-75k\n1486\n\n\nMainline Prot\nDon’t know/refused\n1328\n\n\nCatholic\n$50-75k\n1116\n\n\nMainline Prot\n$50-75k\n1107"
  },
  {
    "objectID": "t4ds/week2.html#pipe-operator",
    "href": "t4ds/week2.html#pipe-operator",
    "title": "R Tidyverse",
    "section": "Pipe operator %>%",
    "text": "Pipe operator %&gt;%\nThe pipe operator %&gt;% allows us to perform a series of functions without storing the outcomes of each function. For example:\nlibrary(dplyr)\nsqrt(log(25))\n[1] 1.794123\n#is the same as\n25 %&gt;% \n  log %&gt;% \n  sqrt\n[1] 1.794123\nWe often start with our data and then apply functions sequentially. The benefit of the pipe operator is more evident when dealing with complex operations."
  },
  {
    "objectID": "t4ds/week2.html#summarizing-data",
    "href": "t4ds/week2.html#summarizing-data",
    "title": "R Tidyverse",
    "section": "Summarizing data",
    "text": "Summarizing data\nOne of the tasks in statistics is to summarize data. Let’s look into this example using data chickwts about Chicken weights and diet. It has two variables weight and feed:\n\n# See what is in the data\nstr(chickwts)\n\n'data.frame':   71 obs. of  2 variables:\n $ weight: num  179 160 136 227 217 168 108 124 143 140 ...\n $ feed  : Factor w/ 6 levels \"casein\",\"horsebean\",..: 2 2 2 2 2 2 2 2 2 2 ...\n\n# Mean and standard deviation of the weight\nchickwts %&gt;% \n  summarise(mean.weight=mean(weight),s.weight=sd(weight))\n\n  mean.weight s.weight\n1    261.3099  78.0737\n\n# Mean and standard deviation of the weight by group\nchickwts %&gt;% \n  group_by(feed) %&gt;% \n  summarise(mean.weight=mean(weight),s.weight=sd(weight), nbr.chick=n())\n\n# A tibble: 6 × 4\n  feed      mean.weight s.weight nbr.chick\n  &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;\n1 casein           324.     64.4        12\n2 horsebean        160.     38.6        10\n3 linseed          219.     52.2        12\n4 meatmeal         277.     64.9        11\n5 soybean          246.     54.1        14\n6 sunflower        329.     48.8        12\n\n# Select groups `casein`, `linseed`, and `soybean`\nchickwts %&gt;% \n  filter(feed %in% c(\"casein\",\"linseed\",\"soybean\")) %&gt;% \n  group_by(feed) %&gt;% \n  summarise(mean.weight=mean(weight),s.weight=sd(weight), nbr.chick=n())\n\n# A tibble: 3 × 4\n  feed    mean.weight s.weight nbr.chick\n  &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;\n1 casein         324.     64.4        12\n2 linseed        219.     52.2        12\n3 soybean        246.     54.1        14"
  },
  {
    "objectID": "t4ds/week2.html#data-visualization-using-ggplot2",
    "href": "t4ds/week2.html#data-visualization-using-ggplot2",
    "title": "R Tidyverse",
    "section": "Data visualization using ggplot2",
    "text": "Data visualization using ggplot2\nggplot2 package is dedicated to data visualization. It can greatly improve the quality and aesthetics of your graphics, and will make you much more efficient in creating them. gg stands for grammar of graphics.\nThis link The R Graph Gallery provides a gallery of graphs created using R. A good place to get inspired and learn some advanced visualizations.\nLet consider the following example:\n\n#Demographic information of midwest counties from 2000 US census\nkable(head(midwest))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPID\ncounty\nstate\narea\npoptotal\npopdensity\npopwhite\npopblack\npopamerindian\npopasian\npopother\npercwhite\npercblack\npercamerindan\npercasian\npercother\npopadults\nperchsd\npercollege\npercprof\npoppovertyknown\npercpovertyknown\npercbelowpoverty\npercchildbelowpovert\npercadultpoverty\npercelderlypoverty\ninmetro\ncategory\n\n\n\n\n561\nADAMS\nIL\n0.052\n66090\n1270.9615\n63917\n1702\n98\n249\n124\n96.71206\n2.5752761\n0.1482826\n0.3767590\n0.1876229\n43298\n75.10740\n19.63139\n4.355859\n63628\n96.27478\n13.151443\n18.01172\n11.009776\n12.443812\n0\nAAR\n\n\n562\nALEXANDER\nIL\n0.014\n10626\n759.0000\n7054\n3496\n19\n48\n9\n66.38434\n32.9004329\n0.1788067\n0.4517222\n0.0846979\n6724\n59.72635\n11.24331\n2.870315\n10529\n99.08714\n32.244278\n45.82651\n27.385647\n25.228976\n0\nLHR\n\n\n563\nBOND\nIL\n0.022\n14991\n681.4091\n14477\n429\n35\n16\n34\n96.57128\n2.8617170\n0.2334734\n0.1067307\n0.2268028\n9669\n69.33499\n17.03382\n4.488572\n14235\n94.95697\n12.068844\n14.03606\n10.852090\n12.697410\n0\nAAR\n\n\n564\nBOONE\nIL\n0.017\n30806\n1812.1176\n29344\n127\n46\n150\n1139\n95.25417\n0.4122574\n0.1493216\n0.4869181\n3.6973317\n19272\n75.47219\n17.27895\n4.197800\n30337\n98.47757\n7.209019\n11.17954\n5.536013\n6.217047\n1\nALU\n\n\n565\nBROWN\nIL\n0.018\n5836\n324.2222\n5264\n547\n14\n5\n6\n90.19877\n9.3728581\n0.2398903\n0.0856751\n0.1028101\n3979\n68.86152\n14.47600\n3.367680\n4815\n82.50514\n13.520249\n13.02289\n11.143211\n19.200000\n0\nAAR\n\n\n566\nBUREAU\nIL\n0.050\n35688\n713.7600\n35157\n50\n65\n195\n221\n98.51210\n0.1401031\n0.1821340\n0.5464022\n0.6192558\n23444\n76.62941\n18.90462\n3.275892\n35107\n98.37200\n10.399635\n14.15882\n8.179287\n11.008586\n0\nAAR\n\n\n\n\n# Get started - `area` and `poptotal` are variable in `midwest`\nggplot(midwest,aes(x=area,y=poptotal))\n\n\n\n\nWhat we see here is a blank ggplot! ggplot does not plot by default a scatter or a line chart! We would need to decide next what should we plot! Let’s make a scatter plot.\n\nggplot(midwest,aes(x=area,y=poptotal)) + \n  geom_point()\n\n\n\n\nYaay! we did it. Next, let’s add a linear regression model: \\(poptotal = \\beta_0 + \\beta_1 area\\).\n\nTo control x and y axis limits, we can use xlim() and ylim() as follows:\n\nNotice that the line we obtain here is different from the line from the first fit (all data included). This happens because ggplot will refit the model lm() to data without the observations that are outside the ranges. This is useful if we want to examine changes in the model line when extreme values (or outliers) are removed.\nWe can also keep the model as the original plot and zoom in using:\n\nLet’s Add some fancy options:\n\nWow! What about adding a new variable to the plot! For example, adding state variable. Let’s change the color to match the state where a data point belongs to; state is a variable in the midwest dataset."
  },
  {
    "objectID": "t4ds/week2.html#and-more",
    "href": "t4ds/week2.html#and-more",
    "title": "R Tidyverse",
    "section": "And more…",
    "text": "And more…\nLessons of this week provide more about tidyverse. The following will be covered more in details:\n\nData manipulation (filter, select, mutate, arrange, summarize, and etc.)\nggplot2 package for data visualization.\nAn extended example  \n\n🛎 🎙️ Recordings on Canvas will cover more details and examples! Have fun learning and coding 😃! Let me know how I can help!"
  },
  {
    "objectID": "t4ds/week2.html#assignment---r-tidyverse",
    "href": "t4ds/week2.html#assignment---r-tidyverse",
    "title": "R Tidyverse",
    "section": "📚 👈 Assignment - R Tidyverse",
    "text": "📚 👈 Assignment - R Tidyverse\nInstructions are posted on Canvas.\n\n\n\n\nWickham, Hadley. 2014. “Tidy Data.” The Journal of Statistical Software 59. http://www.jstatsoft.org/v59/i10/."
  },
  {
    "objectID": "t4ds/week3.html#database",
    "href": "t4ds/week3.html#database",
    "title": "SQL Basics",
    "section": "Database",
    "text": "Database\nA database is an organized collection of data stored and accessed electronically from a computer system. A Database Management System (DBMS) is a software that is used to manage databases.\n\n\n\nData Science\n\n\nIn order to work with data that are stored in databases we need a language. SQL is a standard computer language for relational database management systems (RDBMS). It is used for storing, manipulating and retrieving data in databases.\nSQL has various dialects such as PL/SQL (Oracle), T-SQL (Microsoft), and others.\nIn this course, we will use SQL Server Management Studio hosted at UWF servers. We will use the fictional company Adventure Works data.\nInformation about accessing the SQL Server is posted on Canvas.\n\nBasic concepts\nWhen dealing with databases we will need to know what is:\n\nEntity: is any thing the data represents in a database. For example, Students, Employees, Schools, Departments, etc. There are given as tables.\nData Type: We need to pick a data type for each column when creating a table. There are common data types including INTEGER, FLOAT, CURRENCY, DATE, BOOLEAN, and etc.\nData Definition Language (DDL): DDL commands are used to create or modify database structures. CREATE, ALTER, and DROP are examples of DDL commands.\nData Manipulation Language (DML): DML commands are used to insert, retrieve, or modify data. INSERT, DELETE, and UPDATE are examples of DML commands.\nData Control Language (DCL): DCL commands are used to create rights and permission. GRANT and REVOKE are examples of DCL commands.\nQuery: Data scientists use a query to get data or information from database tables."
  },
  {
    "objectID": "t4ds/week3.html#data-language",
    "href": "t4ds/week3.html#data-language",
    "title": "SQL Basics",
    "section": "Data Language",
    "text": "Data Language\nNow that we have access to SQL server system, we are ready to manipulate some data and execute SQL queries. SQL statements are divided into 3 categories: DDL, DML, and DCL. We can execute SQL queries using SQL Command or using Graphic User Interface (GUI). We shall present next common statements for DDL and DML.\n\nData Definition Language (DDL)\nThe DDL statements are used to create databases and tables. Here is a list of some of the statements:\n\nSQL commands to create a database:\n\n\n\nCREATE DATABASEdatabase_name;\n\n\n\nSQL commands to delete a database:\n\n\n\nDROP DATABASEdatabase_name;\n\n\n⚠️ be very careful to drop databases or tables!\n\nSQL commands to create a Table:\n\n\n\nCREATE TABLEtable_name;\n\n\n\nSQL commands to create a Table from an existing table:\n\n\n\nSELECT... INTOtable_name FROM Orginal_table\n\n\n\nSQL commands to drop a Table:\n\n\n\nDROP TABLEtable_name;\n\n\n\nSQL commands to truncating (remove all records from a table) a Table:\n\n\n\nTRUNCATE TABLEtable_name;\n\n\n\n\nData Maniplulation Language (DML)\nThe DDL statements are used to insert data, update records, and delete records. Data Manipulation Language is used to manipulate data. Here is a list of the main statements:\n\nSQL commands to insert one or more records into a Table:\n\n\n\nINSERT INTOtable_name(col1,col2,...)  VALUES(exp1,exp2,...);\n\n\n\n\nINSERT INTOtable_name  VALUES(exp1,exp2,...);\n\n\n⚠️ Make sure you insert data in the same order as that in the table for the second syntax.\n\nSQL commands to select records from one or more Tables:\n\n\n\nSELECTcolumn(s)  FROMtables WHEREconditions (optional) ORDER BY column(s)ASC | DESC; (optional)\n\n\n\nDISTINCT clause to eliminate duplicates:\n\n\n\nSELECT DISTINCTcolumn_name  FROMtable_name;\n\n\n\nWHERE clause to filter if the condition is true:\n\n\n\nSELECTcolumn(s)  FROMtable_name WHEREconditions;\n\n\n\nArithmetic operators\n\n\n\nSELECTcolumn_name1, column_name2, column_name2*2 AS 'twicecolumn2'  FROMtable_name;\n\n\nBasic arithmetic operators include: %modulo, /division, *multiplication, +addition, and -substraction.\nBasic comparison operators include: =equal to, &lt;&gt;not equal to, &gt;greater than, &gt;=greater than equal to, and more.\nBasic condition operators include: ANDall conditions must be true to get true, ORAny one of the conditions must be true to get true, INtest if an expression matches any value in a list of VALUES, BETWEENcheck if an experession is within a range of VALUES, and more.\n\nORDER BY clause to sort the records:\n\n\n\nSELECTcolumn(s)  FROMtable_name WHEREconditions ORDER BYexpression (by default ASC);\n\n\n\nUPDATE statement to update records:\n\n\n\nUPDATEtable  SETcol1 = value1, col2 = value2, ... WHEREconditions [optional];\n\n\n\nDELETE statement to delete records:\n\n\n\nDELETE FROMtable  WHEREconditions [optional];"
  },
  {
    "objectID": "t4ds/week3.html#functions-and-group-by",
    "href": "t4ds/week3.html#functions-and-group-by",
    "title": "SQL Basics",
    "section": "Functions and GROUP BY",
    "text": "Functions and GROUP BY\nOften you will be asked to answer questions that involve writing queries for summaries using aggregate function and GROUP BY clause.\n\nSQL commands for Aggregate statements:\n\n\n\nSELECT Aggregate Functioncolumn_name  FROMtable_name;\n\n\nBelow are the main aggregate functions:\n\n\n\n\n\n\n\nFunction\nAction\n\n\n\n\nAVG()\naverage values\n\n\nCOUNT()\ncount the number of rows in a table\n\n\nMAX()\nselect the highest value  select the latest date select the last record for a character\n\n\nMIN()\nselect the lowest value  select the earliest date select the first record for a character\n\n\nSUM()\nreturn the total for a numeric column\n\n\nROUND()\nround a number to specific decimal\n\n\n\n\n\n\n\nIn addition to aggregate functions, there are other type of functions:\n-The number functions take a numeric as an input and return a numeric value. The common number functions include CEILING(), FLOOR(), %, POWER(m,n) [\\(m^n\\)], SQRT(), and ROUND().\n-The string functions. The common string functions include CONCAT(), LEFT(), LEN(), LOWER(), REPLACE(), RIGHT(), UPPER(), and SUBSTRING().\n-The Date and Time functions. The common date and time functions include CURRENT_TIMESTAMP(), DATEADD(), DATEPART(), GETDATE(), DATEDIFF(), and SYSDATETIME().\n-The Conversion functions. The common conversion functions include CAST() and CONVERT().\n\nGROUP BY and HAVING Clause:\n\nThe GROUP BY statement is used to group data from a column. HAVING clause is used with a GROUP BY to add conditions on groups.\n\n\nSELECT Aggregate Functioncolumn_name  FROMtable_name WHEREconditions - optional GROUP BYcolumn_name HAVINGconditions - optional ORDER BYcolumn(s) [ASC | DESC] - optional;\n\n\n🛎 🎙️ Recordings on Canvas will cover more details and examples! Have fun learning and coding 😃! Let me know how I can help!"
  },
  {
    "objectID": "t4ds/week3.html#assignments---sql-basics",
    "href": "t4ds/week3.html#assignments---sql-basics",
    "title": "SQL Basics",
    "section": "📚 👈 Assignments - SQL basics",
    "text": "📚 👈 Assignments - SQL basics\nInstructions are posted on Canvas."
  },
  {
    "objectID": "t4ds/week4.html#advanced-sql-commands",
    "href": "t4ds/week4.html#advanced-sql-commands",
    "title": "Advanced SQL",
    "section": "Advanced SQL commands",
    "text": "Advanced SQL commands\nSQL commands to return all rows from two tables:\n\n\nSELECT column(s) FROM table1  UNION ALL SELECT column(s) FROM table2;\n\n\nSQL commands to return only rows that exist in both tables:\n\n\nSELECT column(s) FROM table1  INTERSECT SELECT column(s) FROM table2;\n\n\nSQL commands to return all rows in the first SELECT but excludes those by the second SELECT:\n\n\nSELECT col1,col2,... FROM table1  EXCEPT SELECT col1,col2,... FROM table2;\n\n\nSQL command to specify the number of records to return:\n\n\nSELECTTOPnumber | percent column_names(s) &lt;br/&gt;FROM table_name;`"
  },
  {
    "objectID": "t4ds/week4.html#subqueries",
    "href": "t4ds/week4.html#subqueries",
    "title": "Advanced SQL",
    "section": "Subqueries",
    "text": "Subqueries\nA Subquery is a SQL query nested inside a SQL query. Very useful to create a virtual table usable by the main query.\n\n\nSELECT column(s) FROM table1  WHERE value IN  (SELECT column_name FROM tables2 WHERE conditions);"
  },
  {
    "objectID": "t4ds/week4.html#joins",
    "href": "t4ds/week4.html#joins",
    "title": "Advanced SQL",
    "section": "Joins",
    "text": "Joins\nRelational databases are defined with tables or entities such Employee and Department. To create a link between the two tables a column is defined as Department_ID in both tables. Now, if you would like to extract employee names and departments names you need to SQL JOIN.\nThere are four main type of joins:\n\n\n\n\n\n\n\nJOIN\nAction\n\n\n\n\nINNER JOIN\nreturn records that have matching values in both tables\n\n\nLEFT JOIN\nreturn all records from table1 (LEFT table1) and the matched records from table2\n\n\nRIGHT JOIN\nreturn all records from table2 (RIGHT table1) and the matched records from table1.\n\n\nFULL JOIN()\nreturn all rows from both tables\n\n\n\n\n\n\n\nSyntax:\n\n\nSELECT table1.col_name, table2.col_name FROM table1  INNER JOIN table2ON table1.col_name = table2.col_name;\n\nOR\n\nSELECT table1.col_name, table2.col_name FROM table1  LEFT JOIN table2ON table1.col_name = table2.col_name;\n\nOR\n\nSELECT table1.col_name, table2.col_name FROM table1  RIGHT JOIN table2ON table1.col_name = table2.col_name;\n\nOR\n\nSELECT table1.col_name, table2.col_name FROM table1  FULL JOIN table2ON table1.col_name = table2.col_name;\n\n\n🛎 🎙️ Recordings on Canvas will cover more details and examples! Have fun learning and coding 😃! Let me know how I can help!"
  },
  {
    "objectID": "t4ds/week4.html#assignments---advanced-sql",
    "href": "t4ds/week4.html#assignments---advanced-sql",
    "title": "Advanced SQL",
    "section": "📚 👈 Assignments - Advanced SQL",
    "text": "📚 👈 Assignments - Advanced SQL\nInstructions are posted on Canvas."
  },
  {
    "objectID": "t4ds/week5.html#references",
    "href": "t4ds/week5.html#references",
    "title": "Python Basics - NumPy and Pandas",
    "section": "References",
    "text": "References\n\nPython Data Science Handbook (VanderPlas 2016). Free access\nThink Python (Downey 2015). Free access\nData Science and Analytics with Python (Rogel-Salazar 2018)"
  },
  {
    "objectID": "t4ds/week5.html#introduction-to-python",
    "href": "t4ds/week5.html#introduction-to-python",
    "title": "Python Basics - NumPy and Pandas",
    "section": "Introduction to Python",
    "text": "Introduction to Python\nPython has emerged over the last recent years as one of the most used tools for data science projects. It is known for code readability and interactive features. Similar to R, Python is supported by a large number of packages that extend its features and functions. Common packages are, to name few:\n\nNumPy: provides functions for manipulating arrays\nPandas: provides functions for manipulating data frames\nMatplotlib: provides functions for visualizations and plotting\nStatsmodels: provides functions for statistical models\nScikit-learn: provides functions for machine learning algorithms"
  },
  {
    "objectID": "t4ds/week5.html#getting-started-with-python",
    "href": "t4ds/week5.html#getting-started-with-python",
    "title": "Python Basics - NumPy and Pandas",
    "section": "Getting started with Python",
    "text": "Getting started with Python\nWe will use RStudio IDE to run Python but, there are other IDEs that you may want to check for your information such as Pycharm, Jupyter, and others. We will be using Python 3. We will see that there are multiple similarities between R and Python.\n\nIndentation refers to the spaces at the beginning of a code line. The indentation in Python is very important.\n\nRecordings of this week provide lessons about the following concepts:"
  },
  {
    "objectID": "t4ds/week5.html#python-basics",
    "href": "t4ds/week5.html#python-basics",
    "title": "Python Basics - NumPy and Pandas",
    "section": "Python Basics",
    "text": "Python Basics\n\nPython Variables:\n\n\n# This is Python Code\nprint(\"Hello World!\")\n\nHello World!\n\n\nYou can name a variable following these rules:\n\nOne word\nUse only letters, numbers, and the underscore (_) character\nCan’t begin with a number\nPython is case-sensitive\n\n\nx = \"HeyHey\"\ny = 40\nx\ny\nx, y = \"Hey\", 45 # Assign values to multiple variables\nprint(x)\nprint(y)\nranks = [\"first\",\"second\",\"third\"] # list\nx, y, z = ranks\nprint(ranks)\nx\ny\nz\n\ndef myf():\n  x=\"Hello\"\n  print(x)\n  \nmyf()\n\ndef myf():\n  global x # x to be global - outside the function\n  x=\"Hello\"\n  print(x)\n  \nmyf()\n\nHey\n45\n['first', 'second', 'third']\nHello\nHello\n\n\nData Types:\n\nx = str(3)    # x will be '3'\nx = int(3)    # x will be 3\nx = float(3)  # x is a float - 3.0\nx = 1j       # x is complex\nx = range(5,45)    # x is a range type\nx = [1,2,1,24,54,45,2,1]  # x is a list\nx = (1,2,1,24,54,45,2,1)  # x is a tuple\nx = {\"name\" : \"Ach\", \"age\" : 85}  # x is a dict (mapping)\n\nMath operations:\n\n5+4   # Addition\n5*4   # Multiplication\n5**4  # power / exponent\nprint(\"Hey\"*3) # String operations\nimport math as mt # More more math functions using package *math*\nmt.cos(556) # cosine function\nimport random # generate random numbers\nprint(random.randrange(1, 10))\nimport numpy as np # generate random numbers\nprint(np.random.normal(loc=0,scale=1,size=2))\n\nHeyHeyHey\n9\n[-0.04462282 -0.96572854]\n\n\nStrings operations:\n\nword = \"Hello There!\"\nword[1] # accessing characters in a String\nfor z in word:\n  print(z)\n\nlen(word) # strings length\n\n\"or\" in word # check if \"or\" is in word\nword1 = \"Do you use Python or R or both!\"\n\"or\" in word1 # check if \"or\" is in word1\n\nH\ne\nl\nl\no\n \nT\nh\ne\nr\ne\n!\n\n\nTrue\n\n\nPython assignment operators:\n\n\n\nOperator\nExample\nResults\n\n\n\n\n=\nx = 10\nx = 10\n\n\n+=\nx += 10\nx = x+10\n\n\n-=\nx -= 10\nx = x-10\n\n\n*=\nx *= 10\nx = x*10\n\n\n/=\nx /= 10\nx = x/10\n\n\n%=\nx %= 10\nx = x%10\n\n\n**=\nx **= 10\nx = x**10\n\n\n\n\n\n\n\n\nIf-Else Statements:\n\nh = 2\nif h &gt; 2:\n print(\"Yes!\") # indentation very important other ERROR\nelif h &gt; 50:\n print(\"Yes Yes!\")\nelse:\n  print(\"No\")\n\nNo\n\n\nFor Loop Statements:\n\nfor k in range(1,10): \n  print(str(k)) # does not show up 10; goes up to 9\n\n1\n2\n3\n4\n5\n6\n7\n8\n9"
  },
  {
    "objectID": "t4ds/week5.html#python-numpy",
    "href": "t4ds/week5.html#python-numpy",
    "title": "Python Basics - NumPy and Pandas",
    "section": "Python Numpy",
    "text": "Python Numpy\nNumPy is a Python library. It stands for Numerical Python and very useful for manipulating arrays. It is faster than using Lists and quite useful for machine learning applications.\n\nimport numpy # this code import NumPy library\narr1 =  numpy.array([1,2,45,564,98]) # create array using NumPy\nprint(arr1)\n\n[  1   2  45 564  98]\n\n\nUsually, we give a Library an alias such as np for the NumPy library. Array objects in NumPy are called ndarray. We can pass any array (list, tuple, etc.) to the function array():\n\nimport numpy as np\narr1 = np.array([1,2,45,564,98])\nprint(arr1)\n\n# Multidimensional arrays!\nd0 = np.array(56)\nd1 = np.array([15, 52, 83, 84, 55])\nd2 = np.array([[1, 2, 3], [4, 5, 6]])\nd3 = np.array([[[1, 2, 3], [4, 5, 6]], [[11, 21, 31], [41, 51, 61]]])\n\nprint(d0.ndim) # print dimension\nprint(d1.ndim)\nprint(d2.ndim)\nprint(d3.ndim)\n\n[  1   2  45 564  98]\n0\n1\n2\n3\n\n\nArray Indexing:\n\nimport numpy as np\n\nD2 = np.array([[1,2,3,4,5], [6,7,8,9,10]], dtype=float)\n\nprint('4th element on 1st dim: ', D2[0, 3])\nprint('4th element on 2nd dim: ', D2[1, 3])\nprint('1st dim: ', D2[0, :])\n\narr = np.array([1, 2, 3, 4, 5, 6, 7])\n\nprint(\"From the start to index 2 (not included): \", arr[:2])\nprint(\"From the index 2 (included) to the end: \", arr[2:])\n\n4th element on 1st dim:  4.0\n4th element on 2nd dim:  9.0\n1st dim:  [1. 2. 3. 4. 5.]\nFrom the start to index 2 (not included):  [1 2]\nFrom the index 2 (included) to the end:  [3 4 5 6 7]\n\n\nArithmetic operations and Math/Stat functions:\n\nimport numpy as np\n\na = np.array([[1,2,3,4,5], [6,7,8,9,10]], dtype=\"f\")\nb = np.array([[10,20,30,40,50], [60,70,80,90,100]], dtype=\"i\")\n\nnp.subtract(b,a) # b-a\nnp.add(b,a) # b+a\nnp.divide(b,a) # b/a\nnp.multiply(b,a) # b*a\nnp.exp(a) # exponential function\nnp.log(a) # natural logarithm function\nnp.sqrt(a) # square root function\nnp.full((3,3),5) # 3x3 constant array\na.mean() # mean \na.std() # standard deviation\na.var() # variance\na.mean(axis=0) # mean across axis 0 (rows)\nnp.median(a) # median \nnp.median(a,axis=0) # median \n\narray([3.5, 4.5, 5.5, 6.5, 7.5], dtype=float32)\n\n\nRandom numbers generation:\nRandom is a module in NumPy to offer functions to work with random numbers.\n\nfrom numpy import random\n\nx = random.randint(100) # a random integer from 0 to 100\nprint(x)\n\nx = random.rand(10) # 10 random numbers float from 0 to 1\nprint(x)\n\nx = random.randint(100,size=(10)) # 10 random integers from 0 to 100\nprint(x)\n\nx = random.randint(100,size=(10,10)) # 10x10 random integers from 0 to 100\nprint(x)\n\nx = random.choice([100,12,0,45]) # sample one value from an array\nprint(x)\n\nx = random.choice([100,12,0,45],size=(10)) # sample one value from an array\nprint(x)\n\nx = random.choice([100, 12, 0, 45], p=[0.1, 0.3, 0.6, 0.0], size=(10)) # Probability sampling\nprint(x)\n\nx = random.normal(loc=1, scale=0.5, size=(10)) # Normal distribution\nprint(x)\n\nx = random.normal(loc=1, scale=0.5, size=(10)) # Normal distribution\nprint(x)\n\n64\n[0.62100525 0.2779849  0.21018114 0.26178562 0.39101575 0.64544359\n 0.33894145 0.26154064 0.41172815 0.62376155]\n[ 3 33 17 34 71 47 93 19 53 33]\n[[83 55 63 36 75  0  4 80 70 87]\n [39 73  8 81 45 88  0 56 31 57]\n [67 86 51 54 98 34 37 13  3 71]\n [64 31 96  6 96 18 72 64 78 48]\n [30 32 83 80  1 95 87 85 26 60]\n [18 96 23 96 52 40 76 74 78 35]\n [94 12 59  9  9 20 94  6  4 13]\n [26  3 28 72 58 54 60 41 73 54]\n [81  5 80 70 45  2 40 51 25 23]\n [52 32 32 79 73 52 86 15 46 84]]\n45\n[  0  45 100 100   0   0 100  12   0   0]\n[12  0  0 12 12  0 12  0 12  0]\n[0.72621615 0.72982815 1.83078152 1.54217682 0.97884093 1.32343226\n 2.00349588 0.77152814 1.26080149 0.58117701]\n[0.61940461 0.62148887 1.40658073 0.31354573 0.87145996 0.13589811\n 1.09390334 1.29276541 0.96918963 2.38132619]\n\n\n📚 For more reading visit Introduction to NumPy."
  },
  {
    "objectID": "t4ds/week5.html#python-pandas",
    "href": "t4ds/week5.html#python-pandas",
    "title": "Python Basics - NumPy and Pandas",
    "section": "Python Pandas",
    "text": "Python Pandas\nPandas is a Python library. It is useful for data wrangling and working with data sets. Pandas refers to both Panel Data and Python Data Analysis. This is a handy Cheat Sheet for Pandas for data wrangling.\n\nimport pandas as pd\n\na = [1,6,8]\nseries = pd.Series(a) # this is a panda series\nprint(series)\n\nmydata = {\n  \"calories\": [1000, 690, 190],\n  \"duration\": [50, 40, 20]\n}\nmydataframe = pd.DataFrame(mydata) # data frame\nmydataframe\n\n0    1\n1    6\n2    8\ndtype: int64\n\n\n\n\n\n\n\n\n\ncalories\nduration\n\n\n\n\n0\n1000\n50\n\n\n1\n690\n40\n\n\n2\n190\n20\n\n\n\n\n\n\n\nRead CSV Files\nCSV files are a simple way to store large data sets. Data Frame Pandas can read CSV files easily:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"../datasets/mycars.csv\")\nprint(df.info()) # Info about Data\n\ndf.head()\n\ndf.loc[3,\"speed\"] = np.NaN # insert NaN in the row 10 in speed column\ndf.head()\n\nnewdf = df.dropna() # drop NA cells\nnewdf.head()\n\ndf.dropna(inplace = True) # drop NA cells and replace \"df\" with the new data\ndf.head()\n\ndf = pd.read_csv(\"../datasets/mycars.csv\")\ndf.fillna(100, inplace = True) # replace NA values with 100.\n\ndf[\"speed\"].fillna(10, inplace = True) # replace NA values with 10 only in column \"speed\"\n\nx = df[\"speed\"].mean() # find mean of speed\ndf[\"speed\"].fillna(x, inplace = True) # replace NA values with mean only in column \"speed\"\n\n\nprint(df.duplicated().head()) # show duplicates\ndf.drop_duplicates().head() # drop duplicates\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50 entries, 0 to 49\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype\n---  ------      --------------  -----\n 0   Unnamed: 0  50 non-null     int64\n 1   speed       50 non-null     int64\n 2   dist        50 non-null     int64\ndtypes: int64(3)\nmemory usage: 1.3 KB\nNone\n0    False\n1    False\n2    False\n3    False\n4    False\ndtype: bool\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nspeed\ndist\n\n\n\n\n0\n1\n4\n2\n\n\n1\n2\n4\n10\n\n\n2\n3\n7\n4\n\n\n3\n4\n7\n22\n\n\n4\n5\n8\n16\n\n\n\n\n\n\n\n🛎 🎙️ Recordings on Canvas will cover more details and examples! Have fun learning and coding 😃! Let me know how I can help!"
  },
  {
    "objectID": "t4ds/week5.html#assignments---python-basics",
    "href": "t4ds/week5.html#assignments---python-basics",
    "title": "Python Basics - NumPy and Pandas",
    "section": "📚 👈 Assignments - Python Basics",
    "text": "📚 👈 Assignments - Python Basics\nInstructions are posted on Canvas.\n\n\n\n\nDowney, Allen. 2015. Think Python How to Think Like a Computer Scientist. Green Tea Press Needham, Massachusetts.\n\n\nRogel-Salazar, Jesus. 2018. Data Science and Analytics with Python. CRC Press.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook. O’Reilly Media, Inc."
  },
  {
    "objectID": "t4ds/week6.html#statistical-models-in-python",
    "href": "t4ds/week6.html#statistical-models-in-python",
    "title": "More Python (Stat/ML/Viz)",
    "section": "Statistical Models in Python",
    "text": "Statistical Models in Python\nstatsmodels is a Python package that provides functions for fitting statistical models, conducting statistical tests, and statistical data exploration.\nLet’s read a data set from the list provided in this link. We use the mtcars data set in R package datasets.\n\nimport statsmodels.api as stat # allow to access easily to most of the functions\nimport statsmodels.formula.api as statf # allow to use formula style to fit the models\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = stat.datasets.get_rdataset(\"mtcars\", \"datasets\").data # load data \"mtcars\" from the R package 'datasets'\nprint(df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 32 entries, Mazda RX4 to Volvo 142E\nData columns (total 11 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   mpg     32 non-null     float64\n 1   cyl     32 non-null     int64  \n 2   disp    32 non-null     float64\n 3   hp      32 non-null     int64  \n 4   drat    32 non-null     float64\n 5   wt      32 non-null     float64\n 6   qsec    32 non-null     float64\n 7   vs      32 non-null     int64  \n 8   am      32 non-null     int64  \n 9   gear    32 non-null     int64  \n 10  carb    32 non-null     int64  \ndtypes: float64(5), int64(6)\nmemory usage: 3.0+ KB\nNone\n\nfit_olsregression = statf.ols(\"mpg ~ wt + cyl\",data=df).fit()\nprint(fit_olsregression.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.830\nModel:                            OLS   Adj. R-squared:                  0.819\nMethod:                 Least Squares   F-statistic:                     70.91\nDate:                Fri, 23 Jun 2023   Prob (F-statistic):           6.81e-12\nTime:                        14:31:59   Log-Likelihood:                -74.005\nNo. Observations:                  32   AIC:                             154.0\nDf Residuals:                      29   BIC:                             158.4\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     39.6863      1.715     23.141      0.000      36.179      43.194\nwt            -3.1910      0.757     -4.216      0.000      -4.739      -1.643\ncyl           -1.5078      0.415     -3.636      0.001      -2.356      -0.660\n==============================================================================\nOmnibus:                        4.628   Durbin-Watson:                   1.671\nProb(Omnibus):                  0.099   Jarque-Bera (JB):                3.426\nSkew:                           0.789   Prob(JB):                        0.180\nKurtosis:                       3.287   Cond. No.                         27.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\npred_ols = fit_olsregression.get_prediction()\npred_ols.summary_frame().head()\n\n                        mean   mean_se  ...  obs_ci_lower  obs_ci_upper\nMazda RX4          22.279145  0.601167  ...     16.885964     27.672326\nMazda RX4 Wag      21.465447  0.497629  ...     16.116566     26.814327\nDatsun 710         26.252026  0.725244  ...     20.795395     31.708658\nHornet 4 Drive     20.380516  0.460267  ...     15.045648     25.715384\nHornet Sportabout  16.646958  0.775271  ...     11.161630     22.132285\n\n[5 rows x 6 columns]\n\ndf[\"cyl\"] = df[\"cyl\"].astype(\"category\") # make cyl categorical variable\nfit_olsregression = statf.ols(\"mpg ~ wt + cyl\",data=df).fit() # refit with a categorical variable \nprint(fit_olsregression.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.837\nModel:                            OLS   Adj. R-squared:                  0.820\nMethod:                 Least Squares   F-statistic:                     48.08\nDate:                Fri, 23 Jun 2023   Prob (F-statistic):           3.59e-11\nTime:                        14:31:59   Log-Likelihood:                -73.311\nNo. Observations:                  32   AIC:                             154.6\nDf Residuals:                      28   BIC:                             160.5\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     33.9908      1.888     18.006      0.000      30.124      37.858\ncyl[T.6]      -4.2556      1.386     -3.070      0.005      -7.095      -1.416\ncyl[T.8]      -6.0709      1.652     -3.674      0.001      -9.455      -2.686\nwt            -3.2056      0.754     -4.252      0.000      -4.750      -1.661\n==============================================================================\nOmnibus:                        2.709   Durbin-Watson:                   1.806\nProb(Omnibus):                  0.258   Jarque-Bera (JB):                1.735\nSkew:                           0.559   Prob(JB):                        0.420\nKurtosis:                       3.222   Cond. No.                         18.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nFor more statistics with Python consult the following links: - Statistical tests - Generalized Linear Models - Contingency Tables"
  },
  {
    "objectID": "t4ds/week6.html#machine-learning",
    "href": "t4ds/week6.html#machine-learning",
    "title": "More Python (Stat/ML/Viz)",
    "section": "Machine Learning",
    "text": "Machine Learning\nThe scikit-learn provides function that support machine learning techniques and practices including model fitting, predicting, cross-validation, etc. It also provides various supervised and unsupervised methods. The website of the package is https://scikit-learn.org\n\nLinear models\nFitting regression models is relevant when the target value or response variable is assumed to be a linear combinations of some predictors. The following code will allow you to fit various linear models using sklearn module.\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_percentage_error\n\ndf = stat.datasets.get_rdataset(\"mtcars\", \"datasets\").data # load data \"mtcars\" from the R package 'datasets'\n\n# split data\ntraining_data, testing_data = train_test_split(df, test_size=0.2, random_state=25)\n\n# Create X and Y from training\nY = training_data[\"mpg\"] # response variable / outcome\nX = training_data.drop(columns=[\"mpg\"]) #predictors / features\nreg =  linear_model.LinearRegression().fit(X,Y)\n\n# Create X and Y from testing\nY_test = testing_data[\"mpg\"] # response variable / outcome\nX_test = testing_data.drop(columns=[\"mpg\"]) #predictors / features\nmpg_y_pred = reg.predict(X_test) # predictions\n\nprint(reg.coef_)\n\n[-0.52365917  0.01668511 -0.02157865  0.12362249 -3.46329267  0.70433502\n  1.1100029   3.90616473  0.28676536 -0.16588934]\n\nmean_absolute_percentage_error(y_true=Y_test,y_pred=mpg_y_pred)\n\n0.12447286077371422"
  },
  {
    "objectID": "t4ds/week6.html#visualization-with-python",
    "href": "t4ds/week6.html#visualization-with-python",
    "title": "More Python (Stat/ML/Viz)",
    "section": "Visualization with Python",
    "text": "Visualization with Python\nPython offers multiple tools and libraries that come with a lot of features for data vizualisation and plotting. Among the popular libraries we have:\n\nMatplotlib\nSeaborn\nPlotly\n\nThe matplotlib.pyplot module is a collection of command style functions that make matplotlib work like MATLAB.\n\nA few plots!\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg') # To plot with Markdown\n\nx = np.linspace(0, 10, 100)\nplt.figure();\nplt.plot(x, np.sin(x))\nplt.plot(x, np.cos(x))\nplt.show()\n\n\n\nplt.close()\n\n\n\n\nRead data from sklearn and vizualize\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import load_iris \nimport matplotlib\nmatplotlib.use('Agg') # To plot with Markdown\n\niris = load_iris()\ndf_iris = pd.DataFrame(iris.data)\ndf_iris.columns = iris.feature_names\n\n# Boxplot\nplt.figure();\nplt.boxplot(df_iris)\n\n{'whiskers': [&lt;matplotlib.lines.Line2D object at 0x1305638b0&gt;, &lt;matplotlib.lines.Line2D object at 0x130563b50&gt;, &lt;matplotlib.lines.Line2D object at 0x130578b80&gt;, &lt;matplotlib.lines.Line2D object at 0x130578e20&gt;, &lt;matplotlib.lines.Line2D object at 0x130588e20&gt;, &lt;matplotlib.lines.Line2D object at 0x130594100&gt;, &lt;matplotlib.lines.Line2D object at 0x1305a5100&gt;, &lt;matplotlib.lines.Line2D object at 0x1305a53a0&gt;], 'caps': [&lt;matplotlib.lines.Line2D object at 0x130563df0&gt;, &lt;matplotlib.lines.Line2D object at 0x1305780d0&gt;, &lt;matplotlib.lines.Line2D object at 0x130588100&gt;, &lt;matplotlib.lines.Line2D object at 0x1305883a0&gt;, &lt;matplotlib.lines.Line2D object at 0x1305943a0&gt;, &lt;matplotlib.lines.Line2D object at 0x130594640&gt;, &lt;matplotlib.lines.Line2D object at 0x1305a5640&gt;, &lt;matplotlib.lines.Line2D object at 0x1305a58e0&gt;], 'boxes': [&lt;matplotlib.lines.Line2D object at 0x130563610&gt;, &lt;matplotlib.lines.Line2D object at 0x1305788e0&gt;, &lt;matplotlib.lines.Line2D object at 0x130588b80&gt;, &lt;matplotlib.lines.Line2D object at 0x130594e20&gt;], 'medians': [&lt;matplotlib.lines.Line2D object at 0x130578370&gt;, &lt;matplotlib.lines.Line2D object at 0x130588640&gt;, &lt;matplotlib.lines.Line2D object at 0x1305948e0&gt;, &lt;matplotlib.lines.Line2D object at 0x1305a5b80&gt;], 'fliers': [&lt;matplotlib.lines.Line2D object at 0x130578610&gt;, &lt;matplotlib.lines.Line2D object at 0x1305888e0&gt;, &lt;matplotlib.lines.Line2D object at 0x130594b80&gt;, &lt;matplotlib.lines.Line2D object at 0x1305a5e20&gt;], 'means': []}\n\nplt.xticks([1, 2, 3, 4], iris.feature_names)\n\n([&lt;matplotlib.axis.XTick object at 0x1305308e0&gt;, &lt;matplotlib.axis.XTick object at 0x1305308b0&gt;, &lt;matplotlib.axis.XTick object at 0x1305a5f70&gt;, &lt;matplotlib.axis.XTick object at 0x1305b2790&gt;], [Text(1, 0, 'sepal length (cm)'), Text(2, 0, 'sepal width (cm)'), Text(3, 0, 'petal length (cm)'), Text(4, 0, 'petal width (cm)')])\n\nplt.grid()\nplt.show()\n\n\n\nplt.close()\n\n#  histogram\nplt.figure();\nplt.hist(df_iris.iloc[:,0])\n\n(array([ 9., 23., 14., 27., 16., 26., 18.,  6.,  5.,  6.]), array([4.3 , 4.66, 5.02, 5.38, 5.74, 6.1 , 6.46, 6.82, 7.18, 7.54, 7.9 ]), &lt;BarContainer object of 10 artists&gt;)\n\nplt.show()\n\n\n\nplt.close()\n\n\n\n\n\n\n\n🛎 🎙️ Recordings on Canvas will cover more details and examples! Have fun learning and coding 😃! Let me know how I can help!"
  },
  {
    "objectID": "t4ds/week6.html#assignment---python-statmlviz",
    "href": "t4ds/week6.html#assignment---python-statmlviz",
    "title": "More Python (Stat/ML/Viz)",
    "section": "📚 👈 Assignment - Python Stat/ML/Viz",
    "text": "📚 👈 Assignment - Python Stat/ML/Viz\nInstructions are posted on Canvas."
  },
  {
    "objectID": "t4ds/week7.html#final-exam-project",
    "href": "t4ds/week7.html#final-exam-project",
    "title": "Final Project",
    "section": "📚 👈 Final Exam Project",
    "text": "📚 👈 Final Exam Project\nFinal Project instructions are posted on Canvas."
  },
  {
    "objectID": "t4ds/references.html",
    "href": "t4ds/references.html",
    "title": "References",
    "section": "",
    "text": "Downey, Allen. 2015. Think Python How to Think Like a Computer\nScientist. Green Tea Press Needham, Massachusetts.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r: Write Your\nOwn Functions and Simulations. \" O’Reilly Media, Inc.\".\n\n\nJohn David, Smith, Yang Sophie, Borasky M. Edward (Ed), Tyhurst Jim,\nCame Scott, Thygesen Mary Anne, and Frantz Ian. 2020. Exploring\nEnterprise Databases with r: A Tidyverse Approach. https://smithjd.github.io/sql-pet/.\n\n\nLuraschi, Javier, Kevin Kuo, and Edgar Ruiz. 2019. Mastering Spark\nwith r: The Complete Guide to Large-Scale Analysis and Modeling.\nO’Reilly Media.\n\n\nRogel-Salazar, Jesus. 2018. Data Science and Analytics with\nPython. CRC Press.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook. O’Reilly\nMedia, Inc.\n\n\nWickham, Hadley. 2014. “Tidy Data.” The Journal of\nStatistical Software 59. http://www.jstatsoft.org/v59/i10/.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science:\nImport, Tidy, Transform, Visualize, and Model Data. \" O’Reilly\nMedia, Inc.\".\n\n\nZhang, Preston. 2017. Practical Guide for Oracle SQL, t-SQL and\nMySQL. CRC Press."
  }
]